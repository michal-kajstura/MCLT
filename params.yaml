train:
  method: freeze_part
  tasks:
  languages:
  model: xlm-roberta-base
  loss_func: uniform
  num_repeats: 1
  model_random_state: 2022
  data_random_state: 2022
  train_sample_size: 10000
  num_workers: 8
  gpus: [0]
  max_steps: 10000
  warmup_steps_ratio: 0.1
  batch_size: 16
  accumulate_grad_batches: 1
  max_length: 256
  gradient_checkpointing: false
  freeze_backbone: false
  val_check_interval: 1000
  patience: 5

finetune:
  learning_rate: 0.00001
  batch_size: 16
  accumulate_grad_batches: 1
  gradient_checkpointing: false
  freeze_backbone: false

linear:
  learning_rate: 0.001
  batch_size: 64
  gradient_checkpointing: false
  freeze_backbone: true

freeze_embeddings:
  learning_rate: 0.0005
  batch_size: 16
  gradient_checkpointing: false
  freeze_backbone: embeddings

freeze_part:
  learning_rate: 0.00005
  batch_size: 16
  gradient_checkpointing: false
  freeze_backbone: 4