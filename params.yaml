train:
  method: adapter_tune
  tasks:
  languages:
  model: xlm-roberta-base
  loss_func: uniform
  num_repeats: 5
  model_random_state: 2022
  data_random_state: 2022
  train_sample_size: 10000
  num_workers: 8
  gpus: [0]
  max_steps: 15000
  warmup_steps_ratio: 0.05
  batch_size: 16
  accumulate_grad_batches: 1
  max_length: 256
  gradient_checkpointing: false
  freeze_backbone: false
  val_check_interval: 1000
  patience: 5

finetune:
  learning_rate: 0.00001
  batch_size: 32
  accumulate_grad_batches: 1
  gradient_checkpointing: true
  freeze_backbone: false

linear:
  learning_rate: 0.001
  batch_size: 32
  gradient_checkpointing: false
  freeze_backbone: true

freeze_embeddings:
  learning_rate: 0.0005
  batch_size: 16
  gradient_checkpointing: false
  freeze_backbone: embeddings

freeze_part:
  learning_rate: 0.00001
  batch_size: 16
  gradient_checkpointing: false
  freeze_backbone: 4

adapter:
  learning_rate: 0.0001
  batch_size: 16
  gradient_checkpointing: false

adapter_tune:
  learning_rate: 0.00001
  batch_size: 16
  gradient_checkpointing: false
  adapter_learning_rate: 0.0001
  adapter_finetune_steps_ratio: 0.6
